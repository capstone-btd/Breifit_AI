import asyncio
import json
import random
import sys
from pathlib import Path
import os

from bs4 import BeautifulSoup
from crawl4ai import (
    AsyncWebCrawler,
    BrowserConfig,
    CacheMode,
    CrawlerRunConfig,
)
from crawl4ai.deep_crawling import (
    BFSDeepCrawlStrategy,
    FilterChain,
    URLPatternFilter,
)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ì‚¬ìš©ì ì„¤ì •
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
SEED_URLS = [
    "https://news.naver.com/",
    "https://news.naver.com/main/main.naver?mode=LSD&mid=shm&sid1=100",  # ì •ì¹˜
    "https://news.naver.com/main/main.naver?mode=LSD&mid=shm&sid1=101",  # ê²½ì œ
    "https://news.naver.com/main/main.naver?mode=LSD&mid=shm&sid1=102",  # ì‚¬íšŒ
    "https://news.naver.com/main/main.naver?mode=LSD&mid=shm&sid1=103",  # ìƒí™œ/ë¬¸í™”
    "https://news.naver.com/main/main.naver?mode=LSD&mid=shm&sid1=104",  # ì„¸ê³„
    "https://news.naver.com/main/main.naver?mode=LSD&mid=shm&sid1=105",  # IT/ê³¼í•™
]

BATCH_SIZE = 10
TOTAL_TARGET = 5000
OUT_DIR = Path(os.path.dirname(os.path.abspath(__file__))) / "articles"
OUT_DIR.mkdir(exist_ok=True)

print(f"ì €ì¥ ê²½ë¡œ: {OUT_DIR}")

# ë¸Œë¼ìš°ì € ì„¤ì •
BROWSER_CFG = BrowserConfig(
    headless=True,
    ignore_https_errors=True  # ì¸ì¦ì„œ ì˜¤ë¥˜ ë¬´ì‹œ
)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# í¬ë¡¤ë§ ì „ëµ
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ğŸ†• ê¸°ì‚¬Â·ëŒ“ê¸€ URL í—ˆìš©/ì°¨ë‹¨ íŒ¨í„´
ARTICLE_PATTERNS = [
    # âœ… í—ˆìš©
    "https://n.news.naver.com/mnews/article/*/*",
    "https://n.news.naver.com/mnews/ranking/article/*/*",
    "https://n.news.naver.com/mnews/hotissue/article/*/*",
    "https://n.news.naver.com/article/*/*",
    "https://n.news.naver.com/ranking/article/*/*",
    "https://n.news.naver.com/hotissue/article/*/*",
    "https://news.naver.com/main/read.naver*",
    "https://news.naver.com/article/*/*",
    "https://news.naver.com/ranking/read.naver*",
    "https://news.naver.com/hotissue/article/*/*",

    # ğŸš« ì°¨ë‹¨ (ëŒ“ê¸€ ë·°)
    "!https://n.news.naver.com/*/article/comment/*/*",
    "!https://n.news.naver.com/*/comment/*",
    "!https://news.naver.com/*/article/comment/*/*",
    "!https://news.naver.com/*/comment/*"
]


def make_strategy(limit: int) -> BFSDeepCrawlStrategy:
    """ê¸°ì‚¬ë§Œ ìˆ˜ì§‘í•˜ë„ë¡ í•„í„°ë§í•œ BFS ë”¥ í¬ë¡¤ ì „ëµ"""
    return BFSDeepCrawlStrategy(
        filter_chain=FilterChain([
            URLPatternFilter(patterns=ARTICLE_PATTERNS)
        ]),
        max_depth=5,
        max_pages=limit * 3,  # depthÂ·branch factor ê°ì•ˆ
        include_external=False,
    )

# ë³¸ë¬¸ì„ ì°¾ì„ CSS ì„ íƒì
ARTICLE_SELECTORS = [
    "#dic_area", "#articleBodyContents", "#articeBody", "#articeBodyContents",
    "#newsEndContents", ".article_body", ".article_view",
]

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# í¬ë¡¤ë§ ì½”ì–´
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
async def crawl_naver(start_idx: int, target_count: int, shared_urls: set[str]) -> None:
    """ë„¤ì´ë²„ ê¸°ì‚¬ target_count ê°œ ì €ì¥ (start_idx ë¶€í„°)"""
    saved_urls: set[str] = set()
    saved_count = 0  # ì‹¤ì œ ì €ì¥ëœ íŒŒì¼ ìˆ˜ ì¶”ì 

    run_cfg = CrawlerRunConfig(
        deep_crawl_strategy=make_strategy(target_count),
        cache_mode=CacheMode.BYPASS,
        page_timeout=120_000,
        wait_until="load"         # domcontentloaded ë³´ë‹¤ ì•ˆì „
    )

    async with AsyncWebCrawler(config=BROWSER_CFG, concurrency=10) as crawler:
        batch_no = start_idx // BATCH_SIZE + 1
        print(f"\n[NAVER] ë°°ì¹˜ {batch_no} ì‹œì‘... (ëª©í‘œ: {target_count}ê°œ)")
        seed_url = random.choice(SEED_URLS)
        print(f"ì‹œë“œ URL: {seed_url}")

        results = await crawler.arun(seed_url, config=run_cfg)
        for res in results:
            # ìš”ì²­ ì‹¤íŒ¨ ë˜ëŠ” ì‹œë“œ URL ìì²´ëŠ” ê±´ë„ˆëœ€
            if not res.success or res.url == seed_url:
                continue

            # ì „ì²´ ì‹¤í–‰ ì¤‘ ì´ë¯¸ ì €ì¥ëœ ê¸°ì‚¬ë¼ë©´ ê±´ë„ˆëœ€
            if res.url in shared_urls:
                print(f"  â© ì¤‘ë³µ URL ê±´ë„ˆëœ€: {res.url}")
                continue

            # í˜ì´ì§€ ë³¸ë¬¸ ì¶”ì¶œ
            soup = BeautifulSoup(res.html or "", "lxml")
            article_text = ""
            for sel in ARTICLE_SELECTORS:
                node = soup.select_one(sel)
                if node:
                    for t in node.select("script,style,noscript,iframe"):
                        t.decompose()
                    article_text = node.get_text(" ", strip=True)
                    break

            # ìµœì†Œ ê¸¸ì´ ë¯¸ë‹¬ ì‹œ ê±´ë„ˆëœ€
            if len(article_text) < 100:
                print(f"  â© ë„ˆë¬´ ì§§ì€ ê¸°ì‚¬ ê±´ë„ˆëœ€: {len(article_text)}ì")
                continue

            # ì €ì¥
            idx = start_idx + saved_count
            output_file = OUT_DIR / f"naver_{idx:05d}.json"
            try:
                output_file.write_text(
                    json.dumps({"content": article_text}, ensure_ascii=False),
                    encoding="utf-8",
                )
                saved_count += 1
                print(f"âœ… {idx + 1:5d}/{TOTAL_TARGET}  {res.url}")
                print(f"   ì €ì¥ë¨: {output_file} ({len(article_text)}ì)")
            except Exception as e:
                print(f"âŒ ì €ì¥ ì‹¤íŒ¨: {output_file} - {str(e)}")
                continue

            saved_urls.add(res.url)
            shared_urls.add(res.url)

            if saved_count >= target_count:
                print(f"\nëª©í‘œ ë‹¬ì„±: {saved_count}ê°œ ì €ì¥ ì™„ë£Œ")
                break

    print(f"\n[NAVER] ë°°ì¹˜ {batch_no} ì™„ë£Œ â€“ {saved_count}ê°œ ì €ì¥")
    print(f"í˜„ì¬ê¹Œì§€ ì´ {len(shared_urls)}ê°œì˜ ê³ ìœ  URL ìˆ˜ì§‘")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ë°°ì¹˜ ì‹¤í–‰ í—¬í¼
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def run_batch(batch_num: int, shared_urls: set[str]) -> None:
    """ë‹¨ì¼ ë°°ì¹˜ ì‹¤í–‰ (ë™ê¸° ë˜í¼)"""
    start_idx = batch_num * BATCH_SIZE
    target_count = min(BATCH_SIZE, TOTAL_TARGET - start_idx)
    asyncio.run(crawl_naver(start_idx, target_count, shared_urls))

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ë©”ì¸ ì§„ì…ì 
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
if __name__ == "__main__":
    num_batches = (TOTAL_TARGET + BATCH_SIZE - 1) // BATCH_SIZE
    shared_urls: set[str] = set()

    if len(sys.argv) > 1:
        # ì¸ìˆ˜ë¡œ íŠ¹ì • ë°°ì¹˜ë§Œ ì‹¤í–‰
        batch_num = int(sys.argv[1])
        if 0 <= batch_num < num_batches:
            run_batch(batch_num, shared_urls)
        else:
            print(f"ìœ íš¨í•˜ì§€ ì•Šì€ ë°°ì¹˜ ë²ˆí˜¸ì…ë‹ˆë‹¤. 0-{num_batches - 1} ì‚¬ì´ ì…ë ¥")
    else:
        # ìˆœì°¨ë¡œ ì „ì²´ ë°°ì¹˜ ì‹¤í–‰
        for i in range(num_batches):
            run_batch(i, shared_urls)
