""" This file is for Crwaling Article, News from other webs"""
import asyncio, re, json
from urllib.parse import urljoin, urlparse
from crawl4ai import AsyncWebCrawler, CrawlerRunConfig
from datetime import datetime
from collections import defaultdict
from bs4 import BeautifulSoup

# n.news.naver.com/mnews ë„ë©”ì¸ì˜ ë§í¬ë¥¼ ì°¾ëŠ” ì •ê·œí‘œí˜„ì‹
ARTICLE_REGEX = re.compile(r"^https://n\.news\.naver\.com/mnews/article/\d+/\d+")

# âœ… ìˆ˜ì§‘ ëŒ€ìƒ seed URL(ìš°ë¦¬ê°€ ì •í•˜ëŠ” ë‰´ìŠ¤ ì£¼ì œ í‚¤ì›Œë“œ)
SEED_URLS = [
    "https://news.naver.com/main/ranking/popularDay.naver",  # í™”ì œì„±
    "https://news.naver.com/main/main.naver?mode=LSD&mid=shm&sid1=100",  # ì •ì¹˜
    "https://news.naver.com/main/main.naver?mode=LSD&mid=shm&sid1=101",  # ê²½ì œ
    "https://news.naver.com/main/main.naver?mode=LSD&mid=shm&sid1=102",  # ì‚¬íšŒ
    "https://news.naver.com/main/main.naver?mode=LSD&mid=shm&sid1=103",  # ìƒí™œ/ë¬¸í™”
    "https://news.naver.com/main/main.naver?mode=LSD&mid=shm&sid1=104",  # ì„¸ê³„
    "https://news.naver.com/main/main.naver?mode=LSD&mid=shm&sid1=105",  # IT/ê³¼í•™
]

def analyze_url_pattern(url):
    parsed = urlparse(url)
    path_parts = parsed.path.strip('/').split('/')
    return {
        'domain': parsed.netloc,
        'path_parts': path_parts,
        'query': parsed.query
    }

async def collect_varied_naver_links(limit_per_topic=10):
    article_links_by_topic = defaultdict(set)
    url_patterns = defaultdict(int)

    async with AsyncWebCrawler() as crawler:
        for seed_url in SEED_URLS:
            print(f"\nğŸŒ ìˆ˜ì§‘ ì¤‘: {seed_url}")
            cfg = CrawlerRunConfig(exclude_external_links=True)
            seed_res = await crawler.arun(seed_url, config=cfg)

            if not seed_res.success:
                print(f"âŒ ì‹¤íŒ¨: {seed_url}")
                continue

            print(f"ğŸ“Š ë°œê²¬ëœ ë‚´ë¶€ ë§í¬ ìˆ˜: {len(seed_res.links['internal'])}")
            
            # URL íŒ¨í„´ ë¶„ì„
            for link in seed_res.links["internal"]:
                if "href" in link:
                    full_url = urljoin(seed_url, link["href"])
                    pattern = analyze_url_pattern(full_url)
                    pattern_key = f"{pattern['domain']}/{pattern['path_parts'][0] if pattern['path_parts'] else ''}"
                    url_patterns[pattern_key] += 1

                    if ARTICLE_REGEX.match(full_url):
                        article_links_by_topic[seed_url].add(full_url)
                        print(f"âœ… ê¸°ì‚¬ ë§í¬ ë°œê²¬: {full_url}")

                        if len(article_links_by_topic[seed_url]) >= limit_per_topic:
                            print(f"ğŸ¯ {seed_url} ì£¼ì œì˜ {limit_per_topic}ê°œ ê¸°ì‚¬ ìˆ˜ì§‘ ì™„ë£Œ")
                            break

    # URL íŒ¨í„´ ë¶„ì„ ê²°ê³¼ ì¶œë ¥
    print("\nğŸ“Š URL íŒ¨í„´ ë¶„ì„ ê²°ê³¼:")
    for pattern, count in sorted(url_patterns.items(), key=lambda x: x[1], reverse=True):
        print(f"{pattern}: {count}ê°œ")

    # ëª¨ë“  ì£¼ì œì˜ ë§í¬ë¥¼ í•˜ë‚˜ì˜ ë¦¬ìŠ¤íŠ¸ë¡œ í•©ì¹˜ê¸°
    all_article_links = []
    for topic, links in article_links_by_topic.items():
        all_article_links.extend(list(links))
        print(f"\nğŸ“° {topic} ì£¼ì œ ìˆ˜ì§‘ ê²°ê³¼: {len(links)}ê°œ")

    print(f"\nğŸ“° ìµœì¢… ìˆ˜ì§‘ ê¸°ì‚¬ ë§í¬: {len(all_article_links)}ê°œ")
    if all_article_links:
        with open("naver_links_collected.json", "w", encoding="utf-8") as f:
            json.dump(sorted(all_article_links), f, ensure_ascii=False, indent=2)
        print("ğŸ’¾ ë§í¬ ì €ì¥ ì™„ë£Œ")
    else:
        print("âš ï¸ ìˆ˜ì§‘ëœ ë§í¬ê°€ ì—†ìŠµë‹ˆë‹¤.")

    return all_article_links

async def crawl_article_content(crawler, url):
    try:
        res = await crawler.arun(url)
        if not res.success:
            return None

        soup = BeautifulSoup(res.html, 'html.parser')
        
        # ì œëª© ì¶”ì¶œ
        title = soup.select_one('h2.media_end_head_headline')
        if not title:
            title = soup.select_one('h3.article_title')
        title = title.get_text().strip() if title else ""

        # ë‚´ìš© ì¶”ì¶œ
        content = soup.select_one('div#newsct_article')
        if not content:
            content = soup.select_one('div#articeBody')
        content = content.get_text().strip() if content else ""

        return {
            'url': url,
            'title': title,
            'content': content,
            'crawled_at': datetime.now().isoformat()
        }
    except Exception as e:
        print(f"Error crawling {url}: {str(e)}")
        return None

async def main():
    # ë§í¬ ìˆ˜ì§‘
    article_links = await collect_varied_naver_links()
    print(f"ìˆ˜ì§‘ëœ ê¸°ì‚¬ ë§í¬: {len(article_links)}ê°œ")

    # ê¸°ì‚¬ ë‚´ìš© í¬ë¡¤ë§
    articles = []
    async with AsyncWebCrawler() as crawler:
        for url in article_links:
            article = await crawl_article_content(crawler, url)
            if article:
                articles.append(article)
                print(f"í¬ë¡¤ë§ ì™„ë£Œ: {article['title']}")
                print(f"í¬ë¡¤ë§ ì™„ë£Œ: {article['content']}")

    # ê²°ê³¼ ì €ì¥
    if articles:
        output_file = f"naver_news_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        with open(output_file, "w", encoding="utf-8") as f:
            json.dump(articles, f, ensure_ascii=False, indent=2)
        print(f"\ní¬ë¡¤ë§ ì™„ë£Œ: {len(articles)}ê°œ ê¸°ì‚¬ ì €ì¥ë¨")
        print(f"ì €ì¥ ìœ„ì¹˜: {output_file}")

if __name__ == "__main__":
    asyncio.run(main())
